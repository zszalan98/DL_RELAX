{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.Tokenizers import TokenizersConfig, Tokenizers\n",
    "\n",
    "# load the pre-trained checkpoints\n",
    "checkpoint = torch.load('audio/tokenizers/Tokenizer_iter3_plus_AS2M.pt')\n",
    "\n",
    "cfg = TokenizersConfig(checkpoint['cfg'])\n",
    "BEATs_tokenizer = Tokenizers(cfg)\n",
    "BEATs_tokenizer.load_state_dict(checkpoint['model'])\n",
    "BEATs_tokenizer.eval()\n",
    "\n",
    "# tokenize the audio and generate the labels\n",
    "audio_input_16khz = torch.randn(1, 10000)\n",
    "padding_mask = torch.zeros(1, 10000).bool()\n",
    "\n",
    "labels = BEATs_tokenizer.extract_labels(audio_input_16khz, padding_mask=padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.BEATs import BEATs, BEATsConfig\n",
    "model_path = 'audio/models/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt'\n",
    "# load the pre-trained checkpoints\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "cfg = BEATsConfig(checkpoint['cfg'])\n",
    "BEATs_model = BEATs(cfg)\n",
    "BEATs_model.load_state_dict(checkpoint['model'])\n",
    "BEATs_model.eval()\n",
    "\n",
    "# extract the the audio representation\n",
    "audio_input_16khz = torch.randn(1, 10000)\n",
    "padding_mask = torch.zeros(1, 10000).bool()\n",
    "\n",
    "representation = BEATs_model.extract_features(audio_input_16khz, padding_mask=padding_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "# Replace 'path_to_audio_file.wav' with the path to your actual audio file\n",
    "filename = 'audio/1-9886-A-49.wav'\n",
    "\n",
    "# Load the audio file\n",
    "audio, sample_rate = librosa.load(filename)\n",
    "# If you need to resample to 16 kHz\n",
    "if sample_rate != 16000:\n",
    "    audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000)\n",
    "\n",
    "# convert to tensor\n",
    "audio_input_16khz = torch.from_numpy(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sample_rate = librosa.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 110250])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(audio).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('meta/esc50.csv')\n",
    "# Extract the data from the json file /zhome/58/f/181392/DTU/DL/Project/DL_RELAX/meta/ontology.json\n",
    "import json\n",
    "from pathlib import Path\n",
    "with open('meta/ontology.json', 'r') as f:\n",
    "    ontology = json.load(f)\n",
    "\n",
    "# Create a dictionary mapping the class names to their corresponding indices\n",
    "label_dict = {label['id']: label['name'] for label in ontology}\n",
    "\n",
    "\n",
    "# Get all .wav files from the directory results including subdirectories\n",
    "\n",
    "# Define the directory where the audio files are located\n",
    "audio_dir = \"results\"\n",
    "# Get all .wav files from the directory and its subdirectories\n",
    "selected_filenames = list(Path(audio_dir).rglob('*.wav'))\n",
    "\n",
    "\n",
    "# Define target sample rate and duration\n",
    "target_sample_rate = 16000\n",
    "duration_in_seconds = 5  # Assuming each file is 5 seconds long\n",
    "\n",
    "\n",
    "# Load and process the audio files\n",
    "audio_tensors = []\n",
    "for filename in selected_filenames:    \n",
    "    file_path = os.path.join(filename)\n",
    "    audio, _ = librosa.load(file_path, sr=target_sample_rate, duration=duration_in_seconds)\n",
    "    sd.play(audio, target_sample_rate)\n",
    "    # Wait for the audio to finish playing\n",
    "    sd.wait()\n",
    "    audio_tensors.append(torch.from_numpy(audio))\n",
    "\n",
    "# Stack into a single tensor for batch processing\n",
    "audio_batch = torch.stack(audio_tensors)\n",
    "\n",
    "# audio_batch now has shape (num_audios, target_sample_rate * duration_in_seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 80000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Irregular_Temporal_Frequency_Masking'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_filenames[0].parent.name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irregular_Temporal_Frequency_Masking\n",
      "Audio 1: Real class: rooster\n",
      "Top 3 predictions for audio 1:\n",
      "Prediction 1: Crowing, cock-a-doodle-doo | prob 0.93\n",
      "Prediction 2: Chicken, rooster | prob 0.91\n",
      "Prediction 3: Fowl | prob 0.84\n",
      "------------------------\n",
      "Easy_Frequency\n",
      "Audio 2: Real class: frog\n",
      "Top 3 predictions for audio 2:\n",
      "Prediction 1: Frog | prob 0.95\n",
      "Prediction 2: Croak | prob 0.65\n",
      "Prediction 3: Animal | prob 0.21\n",
      "------------------------\n",
      "Regular_Frequency_Masking\n",
      "Audio 3: Real class: church_bells\n",
      "Top 3 predictions for audio 3:\n",
      "Prediction 1: Change ringing (campanology) | prob 0.69\n",
      "Prediction 2: Church bell | prob 0.14\n",
      "Prediction 3: Bell | prob 0.11\n",
      "------------------------\n",
      "Regular_Temporal_Masking_96890\n",
      "Audio 4: Real class: clock_alarm\n",
      "Top 3 predictions for audio 4:\n",
      "Prediction 1: Alarm clock | prob 0.51\n",
      "Prediction 2: Alarm | prob 0.34\n",
      "Prediction 3: Inside, small room | prob 0.25\n",
      "------------------------\n",
      "Unclear_Classification\n",
      "Audio 5: Real class: chainsaw\n",
      "Top 3 predictions for audio 5:\n",
      "Prediction 1: Chainsaw | prob 0.78\n",
      "Prediction 2: Power tool | prob 0.32\n",
      "Prediction 3: Tools | prob 0.32\n",
      "------------------------\n",
      "Regular_Temporal_Masking_137\n",
      "Audio 6: Real class: keyboard_typing\n",
      "Top 3 predictions for audio 6:\n",
      "Prediction 1: Typing | prob 0.91\n",
      "Prediction 2: Computer keyboard | prob 0.90\n",
      "Prediction 3: Tap | prob 0.02\n",
      "------------------------\n",
      "Irregular_Temporal_Masking\n",
      "Audio 7: Real class: door_wood_creaks\n",
      "Top 3 predictions for audio 7:\n",
      "Prediction 1: Theremin | prob 0.50\n",
      "Prediction 2: Music | prob 0.43\n",
      "Prediction 3: Musical instrument | prob 0.16\n",
      "------------------------\n",
      "Very_Hard_To_Guess\n",
      "Audio 8: Real class: cat\n",
      "Top 3 predictions for audio 8:\n",
      "Prediction 1: Animal | prob 0.77\n",
      "Prediction 2: Meow | prob 0.73\n",
      "Prediction 3: Domestic animals, pets | prob 0.68\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.BEATs import BEATs, BEATsConfig\n",
    "\n",
    "# load the fine-tuned checkpoints\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "cfg = BEATsConfig(checkpoint['cfg'])\n",
    "BEATs_model = BEATs(cfg)\n",
    "BEATs_model.load_state_dict(checkpoint['model'])\n",
    "BEATs_model.eval()\n",
    "\n",
    "# predict the classification probability of each class\n",
    "padding_mask = torch.zeros(audio_batch.shape[0], audio_batch.shape[1]).bool()\n",
    "\n",
    "probs = BEATs_model.extract_features(audio_batch, padding_mask=padding_mask)[0]\n",
    "predictions = {}\n",
    "for i, (top5_label_prob, top5_label_idx) in enumerate(zip(*probs.topk(k=5))):\n",
    "    print(selected_filenames[i].parent.name)\n",
    "    top5_label = [checkpoint['label_dict'][label_idx.item()] for label_idx in top5_label_idx]\n",
    "    # map the label to the corresponding class\n",
    "    top5_label = [label_dict[label] for label in top5_label]\n",
    "    # Store the prediction in a dict for later use, the key is the filename\n",
    "\n",
    "    predictions[selected_filenames[i]] = {'top5_label': top5_label, 'top5_label_prob': top5_label_prob}\n",
    "    # Print it out with the probabilities, then also the real class using the data df\n",
    "    real_class = data.loc[data[\"filename\"] == selected_filenames[i].name, \"category\"].values[0]\n",
    "    print(f'Audio {i+1}: Real class: {real_class}')\n",
    "    print(f'Top 3 predictions for audio {i+1}:')\n",
    "    for j in range(3):\n",
    "        print(f'Prediction {j+1}: {top5_label[j]} | prob {top5_label_prob[j].item():.2f}')\n",
    "    print('------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define the source and destination directories\n",
    "source_dir = 'audio'\n",
    "destination_dir = 'audio/selected'\n",
    "# first remove the existing files in the destination directory (if existing)\n",
    "if os.path.exists(destination_dir):\n",
    "    shutil.rmtree(destination_dir)\n",
    "os.mkdir(destination_dir)\n",
    "\n",
    "# Define target sample rate and duration\n",
    "target_sample_rate = 16000\n",
    "duration_in_seconds = 5  # Assuming each file is 5 seconds long\n",
    "\n",
    "# Load and play the audio files\n",
    "for i, filename in enumerate(selected_filenames):\n",
    "    file_path = os.path.join(source_dir, filename)\n",
    "    source_path = os.path.join(source_dir, filename)\n",
    "    real_class = data.loc[data[\"filename\"] == filename, \"category\"].values[0]\n",
    "    destination_path = os.path.join(destination_dir, f\"{predictions[filename]['top5_label'][0].replace(' ', '')}_{real_class}.wav\")\n",
    "    shutil.copy(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/58/f/181392/DTU/DL/Project/DL_RELAX/beats_env/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "filename = 'audio/sounds/1-9886-A-49.wav'\n",
    "model_path = 'audio/models/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt'\n",
    "_, _, features = extract_features(audio_path=filename, model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 527])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beats_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
