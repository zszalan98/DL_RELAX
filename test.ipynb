{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Another_project_folder\\DL_RELAX\\.venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.Tokenizers import TokenizersConfig, Tokenizers\n",
    "\n",
    "# load the pre-trained checkpoints\n",
    "checkpoint = torch.load('audio/beats/Tokenizer_iter3_plus_AS2M.pt')\n",
    "\n",
    "cfg = TokenizersConfig(checkpoint['cfg'])\n",
    "BEATs_tokenizer = Tokenizers(cfg)\n",
    "BEATs_tokenizer.load_state_dict(checkpoint['model'])\n",
    "BEATs_tokenizer.eval()\n",
    "\n",
    "# tokenize the audio and generate the labels\n",
    "audio_input_16khz = torch.randn(1, 10000)\n",
    "padding_mask = torch.zeros(1, 10000).bool()\n",
    "\n",
    "labels = BEATs_tokenizer.extract_labels(audio_input_16khz, padding_mask=padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.BEATs import BEATs, BEATsConfig\n",
    "model_path = 'audio/beats/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt'\n",
    "# load the pre-trained checkpoints\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "cfg = BEATsConfig(checkpoint['cfg'])\n",
    "BEATs_model = BEATs(cfg)\n",
    "BEATs_model.load_state_dict(checkpoint['model'])\n",
    "BEATs_model.eval()\n",
    "\n",
    "# extract the the audio representation\n",
    "audio_input_16khz = torch.randn(1, 10000)\n",
    "padding_mask = torch.zeros(1, 10000).bool()\n",
    "\n",
    "representation = BEATs_model.extract_features(audio_input_16khz, padding_mask=padding_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_input_16khz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "# Replace 'path_to_audio_file.wav' with the path to your actual audio file\n",
    "filename = 'audio/1-9886-A-49.wav'\n",
    "\n",
    "# Load the audio file\n",
    "audio, sample_rate = librosa.load(filename)\n",
    "# If you need to resample to 16 kHz\n",
    "if sample_rate != 16000:\n",
    "    audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000)\n",
    "\n",
    "# convert to tensor\n",
    "audio_input_16khz = torch.from_numpy(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sample_rate = librosa.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 110250])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(audio).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('meta/esc50.csv')\n",
    "# Extract the data from the json file /zhome/58/f/181392/DTU/DL/Project/DL_RELAX/meta/ontology.json\n",
    "import json\n",
    "with open('meta/ontology.json', 'r') as f:\n",
    "    ontology = json.load(f)\n",
    "\n",
    "# Create a dictionary mapping the class names to their corresponding indices\n",
    "label_dict = {label['id']: label['name'] for label in ontology}\n",
    "\n",
    "\n",
    "# Define the directory where the audio files are located\n",
    "audio_dir = 'audio'\n",
    "\n",
    "# Specify the number of audio files you want to load\n",
    "num_audios = 3  # Or any other number you prefer\n",
    "\n",
    "# Get all .wav files from the directory\n",
    "all_filenames = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
    "\n",
    "# Select a random subset of filenames\n",
    "selected_filenames = random.sample(all_filenames, num_audios)\n",
    "\n",
    "# Define target sample rate and duration\n",
    "target_sample_rate = 16000\n",
    "duration_in_seconds = 5  # Assuming each file is 5 seconds long\n",
    "\n",
    "# Load and process the audio files\n",
    "audio_tensors = []\n",
    "for filename in selected_filenames:    \n",
    "    file_path = os.path.join(audio_dir, filename)\n",
    "    audio, _ = librosa.load(file_path, sr=target_sample_rate, duration=duration_in_seconds)\n",
    "    sd.play(audio, sample_rate)\n",
    "    # Wait for the audio to finish playing\n",
    "    sd.wait()\n",
    "    audio_tensors.append(torch.from_numpy(audio))\n",
    "\n",
    "# Stack into a single tensor for batch processing\n",
    "audio_batch = torch.stack(audio_tensors)\n",
    "\n",
    "# audio_batch now has shape (num_audios, target_sample_rate * duration_in_seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Another_project_folder\\DL_RELAX\\.venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio 1: Real class: rain\n",
      "Top 3 predictions for audio 1:\n",
      "Prediction 1: Rain | prob 0.74\n",
      "Prediction 2: Rain on surface | prob 0.68\n",
      "Prediction 3: Raindrop | prob 0.31\n",
      "------------------------\n",
      "Audio 2: Real class: dog\n",
      "Top 3 predictions for audio 2:\n",
      "Prediction 1: Dog | prob 0.84\n",
      "Prediction 2: Animal | prob 0.81\n",
      "Prediction 3: Domestic animals, pets | prob 0.70\n",
      "------------------------\n",
      "Audio 3: Real class: sea_waves\n",
      "Top 3 predictions for audio 3:\n",
      "Prediction 1: Ocean | prob 0.43\n",
      "Prediction 2: Waves, surf | prob 0.35\n",
      "Prediction 3: Wind | prob 0.20\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.BEATs import BEATs, BEATsConfig\n",
    "\n",
    "# load the fine-tuned checkpoints\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "cfg = BEATsConfig(checkpoint['cfg'])\n",
    "BEATs_model = BEATs(cfg)\n",
    "BEATs_model.load_state_dict(checkpoint['model'])\n",
    "BEATs_model.eval()\n",
    "\n",
    "# predict the classification probability of each class\n",
    "padding_mask = torch.zeros(3, audio_batch.shape[1]).bool()\n",
    "\n",
    "probs = BEATs_model.extract_features(audio_batch, padding_mask=padding_mask)[0]\n",
    "predictions = {}\n",
    "for i, (top5_label_prob, top5_label_idx) in enumerate(zip(*probs.topk(k=5))):\n",
    "    top5_label = [checkpoint['label_dict'][label_idx.item()] for label_idx in top5_label_idx]\n",
    "    # map the label to the corresponding class\n",
    "    top5_label = [label_dict[label] for label in top5_label]\n",
    "    # Store the prediction in a dict for later use, the key is the filename\n",
    "\n",
    "    predictions[selected_filenames[i]] = {'top5_label': top5_label, 'top5_label_prob': top5_label_prob}\n",
    "    # Print it out with the probabilities, then also the real class using the data df\n",
    "    real_class = data.loc[data[\"filename\"] == selected_filenames[i], \"category\"].values[0]\n",
    "    print(f'Audio {i+1}: Real class: {real_class}')\n",
    "    print(f'Top 3 predictions for audio {i+1}:')\n",
    "    for j in range(3):\n",
    "        print(f'Prediction {j+1}: {top5_label[j]} | prob {top5_label_prob[j].item():.2f}')\n",
    "    print('------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define the source and destination directories\n",
    "source_dir = 'audio'\n",
    "destination_dir = 'audio/selected'\n",
    "# first remove the existing files in the destination directory (if existing)\n",
    "if os.path.exists(destination_dir):\n",
    "    shutil.rmtree(destination_dir)\n",
    "os.mkdir(destination_dir)\n",
    "\n",
    "# Define target sample rate and duration\n",
    "target_sample_rate = 16000\n",
    "duration_in_seconds = 5  # Assuming each file is 5 seconds long\n",
    "\n",
    "# Load and play the audio files\n",
    "for i, filename in enumerate(selected_filenames):\n",
    "    file_path = os.path.join(source_dir, filename)\n",
    "    source_path = os.path.join(source_dir, filename)\n",
    "    real_class = data.loc[data[\"filename\"] == filename, \"category\"].values[0]\n",
    "    destination_path = os.path.join(destination_dir, f\"{predictions[filename]['top5_label'][0].replace(' ', '')}_{real_class}.wav\")\n",
    "    shutil.copy(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'audio/1-9886-A-49.wav'\n",
    "model_path = 'audio/beats/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt'\n",
    "features = extract_features(audio_path=filename, model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 527])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beats_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
